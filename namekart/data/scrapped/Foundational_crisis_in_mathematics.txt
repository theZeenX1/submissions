Foundations of mathematics are the logical and mathematical framework that allows the development of mathematics without generating self-contradictory theories, and, in particular, to have reliable concepts of theorems, proofs, algorithms, etc. This may also include the philosophical study of the relation of this framework with reality.[1]

The term "foundations of mathematics" was not coined before the end of the 19th century, although foundations were first established by the ancient Greek philosophers under the name of Aristotle's logic and systematically applied in Euclid's Elements. A mathematical assertion is considered as truth only if it is a theorem that is proved from true premises by means of a sequence of syllogisms (inference rules), the premises being either already proved theorems or self-evident assertions called axioms or postulates.

These foundations were tacitly assumed to be definitive until the introduction of infinitesimal calculus by Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century. This new area of mathematics involved new methods of reasoning and new basic concepts (continuous functions, derivatives, limits) that were not well founded, but had astonishing consequences, such as the deduction from Newton's law of gravitation that the orbits of the planets are ellipses. 

During the 19th century, progress was made towards elaborating precise definitions of the basic concepts of infinitesimal calculus, notably the natural and real numbers. This led, near the end of the 19th century, to a series of seemingly paradoxical mathematical results that challenged the general confidence in the reliability and truth of mathematical results. This has been called the foundational crisis of mathematics. 

The resolution of this crisis involved the rise of a new mathematical discipline called mathematical logic that includes set theory, model theory, proof theory, computability and computational complexity theory, and more recently, parts of computer science. Subsequent discoveries in the 20th century then stabilized the foundations of mathematics into a coherent framework valid for all mathematics. This framework is based on a systematic use of axiomatic method and on set theory, specifically ZFC, the Zermelo–Fraenkel set theory with the axiom of choice.

It results from this that the basic mathematical concepts, such as numbers, points, lines, and geometrical spaces are not defined as abstractions from reality but from basic properties (axioms). Their adequation with their physical origins does not belong to mathematics anymore, although their relation with reality is still used for guiding mathematical intuition: physical reality is still used by mathematicians to choose axioms, find which theorems are interesting to prove, and obtain indications of possible proofs.

Ancient Greece
Most civilisations developed some mathematics, mainly for practical purposes, such as counting (merchants), surveying (delimitation of fields), prosody, astronomy, and astrology. It seems that ancient Greek philosophers were the first to study the nature of mathematics and its relation with the real world.

Zeno of Elea (490 –  c. 430 BC) produced several paradoxes he used to support his thesis that movement does not exist. These paradoxes involve mathematical infinity, a concept that was outside the mathematical foundations of that time and was not well understood before the end of the 19th century.

The Pythagorean school of mathematics originally insisted that the only numbers are natural numbers and ratios of natural numbers. The discovery (around 5th century BC) that the ratio of the diagonal of a square to its side is not the ratio of two natural numbers was a shock to them which they only reluctantly accepted. A testimony of this is the modern terminology of irrational number for referring to a number that is not the quotient of two integers, since "irrational" means originally "not reasonable" or "not accessible with reason".

The fact that length ratios are not represented by rational numbers was resolved by Eudoxus of Cnidus (408–355 BC), a student of Plato, who reduced the comparison of two irrational ratios to comparisons of integer multiples of the magnitudes involved. His method anticipated that of Dedekind cuts in the modern definition of real numbers by Richard Dedekind (1831–1916);[2] see Eudoxus of Cnidus § Eudoxus' proportions.

In the Posterior Analytics, Aristotle (384–322 BC) laid down the logic for organizing a field of knowledge by means of primitive concepts, axioms, postulates, definitions, and theorems. Aristotle took a majority of his examples for this from arithmetic and from geometry, and his logic served as the foundation of mathematics for centuries. This method resembles the modern axiomatic method but with a big philosophical difference: axioms and postulates were supposed to be true, being either self-evident or resulting from experiments, while no other truth than the correctness of the proof is involved in the axiomatic method. So, for Aristotle, a proved theorem is true, while in the axiomatic methods, the proof says only that the axioms imply the statement of the theorem.

Aristotle's logic reached its high point with Euclid's Elements (300 BC), a treatise on mathematics structured with very high standards of rigor: Euclid justifies each proposition by a demonstration in the form of chains of syllogisms (though they do not always conform strictly to Aristotelian templates).
Aristotle's syllogistic logic, together with its exemplification by Euclid's Elements, are recognized as scientific achievements of ancient Greece, and remained as the foundations of mathematics for centuries.

Before infinitesimal calculus
During Middle Ages, Euclid's Elements stood as a perfectly solid foundation for mathematics, and philosophy of mathematics concentrated on the ontological status of mathematical concepts; the question was whether they exist independently of perception (realism) or within the mind only (conceptualism); or even whether they are simply names of collection of individual objects (nominalism). 

In Elements, the only numbers that are considered are natural numbers and ratios of lengths. This geometrical view of non-integer numbers remained dominant until the end of Middle Ages, although the rise of algebra led to consider them independently from geometry, which implies implicitly that there are foundational primitives of mathematics. For example, the transformations of equations introduced by Al-Khwarizmi and the cubic and quartic formulas discovered in the 16th century result from algebraic manipulations that have no geometric counterpart.

Nevertheless, this did not challenge the classical foundations of mathematics since all properties of numbers that were used can be deduced from their geometrical definition.

In 1637, René Descartes published La Géométrie, in which he showed that geometry can be reduced to algebra by means coordinates, which are numbers determining the position of a point. This gives to the numbers that he called real numbers a more foundational role (before him, numbers were defined as the ratio of two lengths). Descartes' book became famous after 1649 and paved the way to infinitesimal calculus.

Infinitesimal calculus
Isaac Newton (1642–1727) in England and Leibniz (1646–1716) in Germany independently developed the infinitesimal calculus for dealing with mobile points (such as planets in the sky) and variable quantities.

This needed the introduction of new concepts such as continuous functions, derivatives and limits. For dealing with these concepts in a logical way, they were defined in terms of infinitesimals that are hypothetical numbers that are infinitely close to zero. The strong implications of infinitesimal calculus on foundations of mathematics is illustrated by a pamphlet of the Protestant philosopher George Berkeley (1685–1753), who wrote "[Infinitesimals] are neither finite quantities, nor quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?".[3]

Also, a lack of rigor has been frequently invoked, because infinitesimals and the associated concepts were not formally defined (lines and planes were not formally defined either, but people were more accustomed to them). Real numbers, continuous functions, derivatives were not formally defined before the 19th century, as well as Euclidean geometry. It is only in the 20th century that a formal definition of infinitesimals has been given, with the proof that the whole infinitesimal can be deduced from them.

Despite its lack of firm logical foundations, infinitesimal calculus was quickly adopted by mathematicians, and validated by its numerous applications; in particular the fact that the planet trajectories can be deduced from the Newton's law of gravitation.

19th century
In the 19th century, mathematics developed quickly in many directions. Several of the problems that were considered led to questions on the foundations of mathematics. Frequently, the proposed solutions led to further questions that were often simultaneously of philosophical and mathematical nature. All these questions led, at the end of the 19th century and the beginning of the 20th century, to debates which have been called the foundational crisis of mathematics. The following subsections describe the main such foundational problems revealed during the 19th century.

Real analysis
Cauchy (1789–1857) started the project of giving rigorous bases to infinitesimal calculus. In particular, he rejected the heuristic principle that he called the generality of algebra, which consisted to apply properties of algebraic operations to infinite sequences without proper proofs. In his Cours d'Analyse (1821), he considered very small quantities, which could presently be called "sufficiently small quantities"; that is, a sentence such that "if x is very small then ..." must be understood as "there is a (sufficiently large) natural number n such that |x| < 1/n". In the proofs he used this in a way that predated the modern (ε, δ)-definition of limit.[4]

The modern (ε, δ)-definition of limits and continuous functions was first developed by Bolzano in 1817, but remained relatively unknown, and Cauchy probably did know Bolzano's work.

Karl Weierstrass (1815–1897) formalized and popularized the (ε, δ)-definition of limits, and discovered some pathological functions that seemed paradoxical at this time, such as continuous, nowhere-differentiable functions. Indeed, such functions contradict previous conceptions of a function as a rule for computation or a smooth graph. 

At this point, the program of arithmetization of analysis (reduction of mathematical analysis to arithmetic and algebraic operations) advocated by Weierstrass was essentially completed, except for two points.

Firstly, a formal definition of real numbers was still lacking. Indeed, beginning with Richard Dedekind in 1858, several mathematicians worked on the definition of the real numbers, including Hermann Hankel, Charles Méray, and Eduard Heine, but this is only in 1872 that two independent complete definitions of real numbers were published: one by Dedekind, by means of Dedekind cuts; the other one by Georg Cantor as equivalence classes of Cauchy sequences.[5]

Several problems were left open by these definitions, which contributed to the foundational crisis of mathematics. Firstly both definitions suppose that rational numbers and thus natural numbers are rigorously defined; this was done a few years later with Peano axioms. Secondly, both definitions involve infinite sets (Dedekind cuts and sets of the elements of a Cauchy sequence), and Cantor's set theory was published several years later.

The third problem is more subtle: and is related to the foundations of logic: classical logic is a first order logic; that is, quantifiers apply to variables representing individual elements, not to variables representing (infinite) sets of elements. The basic property of the completeness of the real numbers that is required for defining and using real numbers involves a quantification on infinite sets. Indeed, this property may be expressed either as for every infinite sequence of real numbers, if it is a Cauchy sequence, it has a limit that is a real number, or as every subset of the real numbers that is bounded has a least upper bound that is a real number. This need of quantification over infinite sets is one of the motivation of the development of higher-order logics during the first half of the 20th century.

Non-Euclidean geometries
Before the 19th century, there were many failed attempts to derive the parallel postulate from other axioms of geometry. In an attempt to prove that its negation leads to a contradiction, Johann Heinrich Lambert (1728–1777) started to build hyperbolic geometry and introduced the hyperbolic functions and computed the area of a hyperbolic triangle (where the sum of angles is less than 180°). 

Continuing the construction of this new geometry, several mathematicians proved independently that if it is inconsistent, then Euclidean geometry is also inconsistent and thus that the parallel postulate cannot be proved. This was proved by Nikolai Lobachevsky in 1826, János Bolyai (1802–1860) in 1832 and Carl Friedrich Gauss (unpublished).

Later in the 19th century, the German mathematician Bernhard Riemann developed Elliptic geometry, another non-Euclidean geometry where no parallel can be found and the sum of angles in a triangle is more than 180°. It was proved consistent by defining points as pairs of antipodal points on a sphere (or hypersphere), and lines as great circles on the sphere. 

These proofs of unprovability of the parallel postulate lead to several philosophical problems, the main one being that before this discovery, the parallel postulate and all its consequences were considered as true. So, the non-Euclidean geometries challenged the concept of mathematical truth.

Synthetic vs. analytic geometry
Since the introduction of analytic geometry by René Descartes in the 17th century, there were two approaches to geometry, the old one called synthetic geometry, and the new one, where everything is specified in terms of real numbers called coordinates.

Mathematicians did not worry much about the contradiction between these two approaches before the mid-nineteenth century, where there was "an acrimonious controversy between the proponents of synthetic and analytic methods in projective geometry, the two sides accusing each other of mixing projective and metric concepts".[6] Indeed, there is no concept of distance in a projective space, and the cross-ratio, which is a number, is a basic concept of synthetic projective geometry. 

Karl von Staudt developed a purely geometric approach to this problem by introducing "throws" that form what is presently called a field, in which the cross ratio can be expressed.

Apparently, the problem of the equivalence between analytic and synthetic approach was completely solved only with Emil Artin's book Geometric Algebra published in 1957. It was well known that, given a field k, one may define affine and projective spaces over k in terms of k-vector spaces. In these spaces, the Pappus hexagon theorem holds. Conversely, if the Pappus hexagon theorem is included in the axioms of a plane geometry, then one can define a field k such that the geometry is the same as the affine or projective geometry over k.

Natural numbers
The work of making rigorous real analysis and the definition of real numbers, consisted of reducing everything to rational numbers and thus to natural numbers, since positive rational numbers are fractions of natural numbers. There was therefore a need of a formal definition of natural numbers, which imply as axiomatic theory of arithmetic. This was started with Charles Sanders Peirce in 1881 and Richard Dedekind in 1888, who defined a natural numbers as the cardinality of a finite set.[citation needed]. However, this involves set theory, which was not formalized at this time.

Giuseppe Peano provided in 1888 a complete axiomatisation based on the ordinal property of the natural numbers. The last Peano's axiom is the only one that induces logical difficulties, as it begin with either "if S is a set then" or "if 



φ


{\displaystyle \varphi }

 is a predicate then". So, Peano's axioms induce a quantification on infinite sets, and this means that Peano arithmetic is what is presently called a Second-order logic.

This was not well understood at that times, but the fact that infinity occurred in the definition of the natural numbers was a problem for many mathematicians of this time. For example, Henri Poincaré stated that axioms can only be demonstrated in their finite application, and concluded that it is "the power of the mind" which allows conceiving of the indefinite repetition of the same act.[7] This applies in particular to the use of the last Peano axiom for showing that the successor function generates all natural numbers. Also, Leopold Kronecker said "God made the integers, all else is the work of man".[a] This may be interpreted as "the integers cannot be mathematically defined".

Infinite sets
Before the second half of the 19th century, infinity was a philosophical concept that did not belong to mathematics. However, with the rise of infinitesimal calculus, mathematicians became to be accustomed  to infinity, mainly through potential infinity, that is, as the result of an endless process, such as the definition of an infinite sequence, an infinite series or a limit. The possibility of an actual infinity was the subject of many philosophical disputes.

Sets, and more specially infinite sets were not considered as a mathematical concept; in particular, there was no fixed term for them. A dramatic change arose with the work of Georg Cantor who was the first mathematician to systematically study infinite sets. In particular, he introduced cardinal numbers that measure the size of infinite sets, and ordinal numbers that, roughly speaking, allow one to continue to count after having reach infinity. One of his major results is the discovery that there are strictly more real numbers than natural numbers (the cardinal of the continuum of the real numbers is greater than that of the natural numbers).

These results were rejected by many mathematicians and philosophers, and led to debates that are a part of the foundational crisis of mathematics.

The crisis was amplified with the Russel's paradox that asserts that the phrase "the set of all sets" is self-contradictory. This condradiction introduced a doubt on the consistency of all mathematics.

With the introduction of the Zermelo–Fraenkel set theory (c. 1925) and its adoption by the mathematical community, the doubt about the consistency was essentially removed, although consistency of set theory cannot be proved because of Gödel's incompleteness theorem.

Mathematical logic
In 1847, De Morgan published his laws and George Boole devised an algebra, now called Boolean algebra,   that allows expressing Aristotle's logic in terms of formulas and algebraic operations. Boolean algebra is the starting point of mathematization  logic and the basis of propositional calculus

Independently, in the 1870's, Charles Sanders Peirce and Gottlob Frege extended propositional calculus by introducing  quantifiers, for building predicate logic.

Frege pointed out three desired properties of a logical theory:[citation needed]consistency (impossibility of proving contradictory statements), completeness (any statement is either provable or refutable; that is, its negation is provable), and decidability (there is a decision procedure to test every statement).

By near the turn of the century, Bertrand Russell popularized Frege's work and discovered Russel's paradox which implies that the phrase "the set of all sets" is self-contradictory. This paradox seemed to make the whole mathematics inconsistent  and is one of the major causes of the foundational crisis of mathematics.

Foundational crisis
The foundational crisis of mathematics
arose at the end of the 19th century and the beginning of the 20th century with the discovery of several paradoxes or counter-intuitive results. 

The first one was the proof that the parallel postulate cannot be proved. This results from a construction of a Non-Euclidean geometry inside Euclidean geometry, whose inconsistency would imply the inconsistency of Euclidean geometry. A well known paradox is Russell's paradox, which shows that the phrase "the set of all sets that do not contain themselves" is self-contradictory. Other philosophical problems were the proof of the existence of mathematical objects that cannot be computed or explicitly described, and the proof of the existence of theorems of arithmetic that cannot be proved with Peano arithmetic. 

Several schools of philosophy of mathematics were challenged with these problems in the 20th century, and are described below. 

These problems were also studied by mathematicians, and this led to establish mathematical logic as a new area of mathematics, consisting of providing mathematical definitions to logics (sets of inference rules), mathematical and logical theories, theorems, and proofs, and of using mathematical methods to prove theorems about these concepts. 

This led to unexpected results, such as Gödel's incompleteness theorems, which, roughly speaking, assert that, if a theory contains the standard arithmetic, it cannot be used to prove that it itself is not self-contradictory; and, if it is not self-contradictory, there are theorems that cannot be proved inside the theory, but are nevertheless true in some technical sense. 

Zermelo–Fraenkel set theory with the axiom of choice (ZFC) is a logical theory established by Ernst Zermelo and Abraham Fraenkel. It became the standard foundation of modern mathematics, and, unless the contrary is explicitly specified, it is used in all modern mathematical texts, generally implicitly.

Simultaneously, the axiomatic method became a de facto standard: the proof of a theorem must result from explicit axioms and previously proved theorems by the application of clearly defined inference rules. The axioms need not correspond to some reality. Nevertheless, it is an open philosophical problem to explain why the axiom systems that lead to rich and useful theories are those resulting from abstraction from the physical reality or other mathematical theory.

In summary, the foundational crisis is essentially resolved, and this opens new philosophical problems. In particular, it cannot be proved that the new foundation (ZFC) is not self-contradictory. It is a general consensus that, if this would happen, the problem could be solved by a mild modification of ZFC. 

Philosophical views
When the foundational crisis arose, there was much debate among mathematicians and logicians about what should be done for restoring confidence in mathematics. This involved philosophical questions about mathematical truth, the relationship of mathematics with reality, the reality of mathematical objects, and the nature of mathematics.

For the problem of foundations, there was two main options for trying to avoid paradoxes. The first one led to intuitionism and constructivism, and consisted to restrict the logical rules for remaining closer to intuition, while the second, which has been called formalism, considers that a theorem is true if it can be deduced from axioms by applying inference rules (formal proof), and that no "trueness" of the axioms is needed for the validity of a theorem.

It has been claimed[by whom?] that formalists, such as David Hilbert (1862–1943), hold that mathematics is only a language and a series of games. Hilbert insisted that formalism, called "formula game" by him, is a fundamental part of mathematics, but that mathematics must not be reduced to formalism. Indeed, he used the words "formula game" in his 1927 response to L. E. J. Brouwer's criticisms:

And to what extent has the formula game thus made possible been successful? This formula game enables us to express the entire thought-content of the science of mathematics in a uniform manner and develop it in such a way that, at the same time, the interconnections between the individual propositions and facts become clear ... The formula game that Brouwer so deprecates has, besides its mathematical value, an important general philosophical significance. For this formula game is carried out according to certain definite rules, in which the technique of our thinking is expressed. These rules form a closed system that can be discovered and definitively stated.[10]
Thus Hilbert is insisting that mathematics is not an arbitrary game with arbitrary rules; rather it must agree with how our thinking, and then our speaking and writing, proceeds.[10]

We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.[11]
The foundational philosophy of formalism, as exemplified by David Hilbert, is a response to the paradoxes of set theory, and is based on formal logic. Virtually all mathematical theorems today can be formulated as theorems of set theory. The truth of a mathematical statement, in this view, is represented by the fact that the statement can be derived from the axioms of set theory using the rules of formal logic.

Merely the use of formalism alone does not explain several issues: why we should use the axioms we do and not some others, why we should employ the logical rules we do and not some others, why "true" mathematical statements (e.g., the laws of arithmetic) appear to be true, and so on. Hermann Weyl posed these very questions to Hilbert:

What "truth" or objectivity can be ascribed to this theoretic construction of the world, which presses far beyond the given, is a profound philosophical problem. It is closely connected with the further question: what impels us to take as a basis precisely the particular axiom system developed by Hilbert? Consistency is indeed a necessary but not a sufficient condition. For the time being we probably cannot answer this question ...[12]
In some cases these questions may be sufficiently answered through the study of formal theories, in disciplines such as reverse mathematics and computational complexity theory. As noted by Weyl, formal logical systems also run the risk of inconsistency; in Peano arithmetic, this arguably has already been settled with several proofs of consistency, but there is debate over whether or not they are sufficiently finitary to be meaningful. Gödel's second incompleteness theorem establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. What Hilbert wanted to do was prove a logical system S was consistent, based on principles P that only made up a small part of S. But Gödel proved that the principles P could not even prove P to be consistent, let alone S.

Intuitionists, such as L. E. J. Brouwer (1882–1966), hold that mathematics is a creation of the human mind. Numbers, like fairy tale characters, are merely mental entities, which would not exist if there were never any human minds to think about them.

The foundational philosophy of intuitionism or constructivism, as exemplified in the extreme by Brouwer and Stephen Kleene, requires proofs to be "constructive" in nature –  the existence of an object must be demonstrated rather than inferred from a demonstration of the impossibility of its non-existence. For example, as a consequence of this the form of proof known as reductio ad absurdum is suspect.

Some modern theories in the philosophy of mathematics deny the existence of foundations in the original sense. Some theories tend to focus on mathematical practice, and aim to describe and analyze the actual working of mathematicians as a social group. Others try to create a cognitive science of mathematics, focusing on human cognition as the origin of the reliability of mathematics when applied to the real world. These theories would propose to find foundations only in human thought, not in any objective outside construct. The matter remains controversial.

Logicism is a school of thought, and research programme, in the philosophy of mathematics, based on the thesis that mathematics is an extension of logic or that some or all mathematics may be derived in a suitable formal system whose axioms and rules of inference are 'logical' in nature. Bertrand Russell and Alfred North Whitehead championed this theory initiated by Gottlob Frege and influenced by Richard Dedekind.

Many researchers in axiomatic set theory have subscribed to what is known as set-theoretic Platonism, exemplified by Kurt Gödel.

Several set theorists followed this approach and actively searched for axioms that may be considered as true for heuristic reasons and that would decide the continuum hypothesis. Many large cardinal axioms were studied, but the hypothesis always remained independent from them and it is now considered unlikely that CH can be resolved by a new large cardinal axiom. Other types of axioms were considered, but none of them has reached consensus on the continuum hypothesis yet. Recent work by Hamkins proposes a more flexible alternative: a set-theoretic multiverse allowing free passage between set-theoretic universes that satisfy the continuum hypothesis and other universes that do not.

This argument by Willard Quine and Hilary Putnam says (in Putnam's shorter words),

... quantification over mathematical entities is indispensable for science ... therefore we should accept such quantification; but this commits us to accepting the existence of the mathematical entities in question.
However, Putnam was not a Platonist.

Few mathematicians are typically concerned on a daily, working basis over logicism, formalism or any other philosophical position. Instead, their primary concern is that the mathematical enterprise as a whole always remains productive. Typically, they see this as ensured by remaining open-minded, practical and busy; as potentially threatened by becoming overly-ideological, fanatically reductionistic or lazy.

Such a view has also been expressed by some well-known physicists.

For example, the Physics Nobel Prize laureate Richard Feynman said

People say to me, "Are you looking for the ultimate laws of physics?" No, I'm not ... If it turns out there is a simple ultimate law which explains everything, so be it – that would be very nice to discover. If it turns out it's like an onion with millions of layers ... then that's the way it is. But either way there's Nature and she's going to come out the way She is. So therefore when we go to investigate we shouldn't predecide what it is we're looking for only to find out more about it.[13]
And Steven Weinberg:[14]

The insights of philosophers have occasionally benefited physicists, but generally in a negative fashion – by protecting them from the preconceptions of other philosophers. ... without some guidance from our preconceptions one could do nothing at all. It is just that philosophical principles have not generally provided us with the right preconceptions.
Weinberg believed that any undecidability in mathematics, such as the continuum hypothesis, could be potentially resolved despite the incompleteness theorem, by finding suitable further axioms to add to set theory.

Gödel's completeness theorem establishes an equivalence in first-order logic between the formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an "explicit construction" of a model described by the theory; this model will be countable if the language of the theory is countable. However this "explicit construction" is not algorithmic. It is based on an iterative process of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).

More paradoxes
The following lists some notable results in metamathematics. Zermelo–Fraenkel set theory is the most widely studied axiomatization of set theory. It is abbreviated ZFC when it includes the axiom of choice and ZF when the axiom of choice is excluded.

Toward resolution of the crisis
Starting in 1935, the Bourbaki group of French mathematicians started publishing a series of books to formalize many areas of mathematics on the new foundation of set theory.

The intuitionistic school did not attract many adherents, and it was not until Bishop's work in 1967 that constructive mathematics was placed on a sounder footing.[16]

One may consider that Hilbert's program has been partially completed, so that the crisis is essentially resolved, satisfying ourselves with lower requirements than Hilbert's original ambitions. His ambitions were expressed in a time when nothing was clear: it was not clear whether mathematics could have a rigorous foundation at all.

There are many possible variants of set theory, which differ in consistency strength, where stronger versions (postulating higher types of infinities) contain formal proofs of the consistency of weaker versions, but none contains a formal proof of its own consistency. Thus the only thing we do not have is a formal proof of consistency of whatever version of set theory we may prefer, such as ZF.

In practice, most mathematicians either do not work from axiomatic systems, or if they do, do not doubt the consistency of ZFC, generally their preferred axiomatic system. In most of mathematics as it is practiced, the incompleteness and paradoxes of the underlying formal theories never played a role anyway, and in those branches in which they do or whose formalization attempts would run the risk of forming inconsistent theories (such as logic and category theory), they may be treated carefully.

The development of category theory in the middle of the 20th century showed the usefulness of set theories guaranteeing the existence of larger classes than does ZFC, such as Von Neumann–Bernays–Gödel set theory or Tarski–Grothendieck set theory, albeit that in very many cases the use of large cardinal axioms or Grothendieck universes is formally eliminable.

One goal of the reverse mathematics program is to identify whether there are areas of "core mathematics" in which foundational issues may again provoke a crisis.

See also
Notes
References
External links
