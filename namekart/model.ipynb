{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrape example for Wikipedia pages\n",
    "\n",
    "The first step is to scrape data which will be used to train our language model.\n",
    "\n",
    "Data sources depend heavily on the type of model that one is trying to build. For eg., if one wants to create a language model that predicts the next word in the case of autocompletion tasks, they must use data obtained from chatrooms, stories, and social media posts.\n",
    "\n",
    "For general next word predictors, a huge corpus of well written text needs to be used, so that the model is well acquinted in different topics, as well as has a large \"look-up table\" to refer to.\n",
    "\n",
    "For this task, we will be scrapping data for a next word predictor, not specifically a \"chat-bot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_html (url:str):\n",
    "    r = requests.get(url)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages downloaded: 200"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DATA_FOLDER = './data'\n",
    "CREATE_DATASET = True\n",
    "PAGES_LIMIT = 200\n",
    "\n",
    "num_links = 0\n",
    "pages = [] # not to go through the same pages\n",
    "def get_texts_from_wikipedia(link:str, idx:int, depth:int=3, limit:int=PAGES_LIMIT):\n",
    "    \"\"\"\n",
    "    Recursively goes through all wiki links in the wikipedia article until a given depth\n",
    "    \n",
    "    Warning: Make sure to set appropriate depth based on network bandwidth and storage\n",
    "    \"\"\"\n",
    "    global num_links\n",
    "    if idx == depth or num_links >= limit:\n",
    "        return\n",
    "    html = get_html(link)\n",
    "    bs = soup(html, 'html.parser')\n",
    "    content_div = bs.find(\"div\", { \"class\": \"mw-content-ltr\" }) # The main container for the content\n",
    "    accepted_tags = [\"h1\", \"h2\", \"h3\", \"p\"]                     # Elements inside the container with relevant content\n",
    "    \n",
    "    if not content_div:\n",
    "        return\n",
    "    \n",
    "    text = \"\"\n",
    "    anchors = []\n",
    "    for element in content_div.find_all(True):\n",
    "        if element.name in accepted_tags:\n",
    "            text += element.text + '\\n'\n",
    "        if element.name == \"a\" and element.get(\"href\") and element.get(\"href\").strip().startswith(\"/wiki/\"):\n",
    "            anchors.append(element.get(\"href\").strip())\n",
    "    \n",
    "    save_text_file = os.path.join(DATA_FOLDER, 'scrapped/', link.split('/')[-1] + \".txt\")\n",
    "    with open(save_text_file, \"w\") as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    num_links += 1\n",
    "    print(f\"\\rNumber of pages downloaded: {num_links}\", end=\"\", flush=True)\n",
    "    \n",
    "    # go through links\n",
    "    for a in anchors:\n",
    "        if a in pages or any(invalid_link in a for invalid_link in [\"File:\", \"Special:\", \"Wikipedia:\", \"Help:\", \"Talk:\", \"Category:\", \"Portal:\", \"Template:\"]):\n",
    "            continue\n",
    "        pages.append(a)\n",
    "        next_link = os.path.join(\"https://en.wikipedia.org/\" + a)\n",
    "        get_texts_from_wikipedia(next_link, idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "topics = [\n",
    "    \"https://en.wikipedia.org/wiki/Paleontology\",\n",
    "    \"https://en.wikipedia.org/wiki/Outer_space\",\n",
    "    \"https://en.wikipedia.org/wiki/Bell_Labs\",\n",
    "]\n",
    "\n",
    "if CREATE_DATASET:\n",
    "    shutil.rmtree(DATA_FOLDER, ignore_errors=True)\n",
    "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_FOLDER, 'scrapped/'), exist_ok=True)\n",
    "    \n",
    "    for topic in topics:\n",
    "        get_texts_from_wikipedia(topic, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "This step involves taking data and reviewing data and filtering out unnecessary data or invalid characters.\n",
    "\n",
    "After that, we need to tokenize the data. This can be done by using encoders like BPE and then training a model to create task related token. Tokenization is very important as it defines the language by which the model \"talks\". If optimal length tokens are not present or necessary number of tokens are not present, then the model might correspond to \"bloated\" input (token length very small) or isn't flexible enough (too many word length tokens).\n",
    "\n",
    "Last step involves structuring the data in a way that is acceptable by the model.\n",
    "\n",
    "Since this is a general language model, we can skip the filterring part, and move on to tokenization. We will be using tiktoken for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import get_encoding\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DATA_FOLDER = './data/scrapped'\n",
    "TOKENS_FOLDER = './data/tokens'\n",
    "DATASET_FOLDER = './data/dataset'\n",
    "\n",
    "shutil.rmtree(TOKENS_FOLDER, ignore_errors=True)\n",
    "os.makedirs(TOKENS_FOLDER, exist_ok=True)\n",
    "shutil.rmtree(DATASET_FOLDER, ignore_errors=True)\n",
    "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
    "\n",
    "# Encoding used by GPT-4\n",
    "enc = get_encoding('cl100k_base')\n",
    "df = pd.DataFrame(columns=['location', 'text', 'encoded_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(os.listdir(INPUT_DATA_FOLDER)):\n",
    "    text_file_loc = os.path.join(INPUT_DATA_FOLDER, filename)\n",
    "    \n",
    "    text = \"\"\n",
    "    with open(text_file_loc, \"r\") as text_file:\n",
    "        text = text_file.read()\n",
    "    \n",
    "    encoded_text = enc.encode(text=text)\n",
    "    \n",
    "    df.loc[i] = [ filename, text, encoded_text ]\n",
    "\n",
    "df.to_csv(os.path.join(TOKENS_FOLDER, 'tokens.csv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset\n",
    "\n",
    "In the above section, we saw how to tokenize the input data easily using tiktoken. But, we still haven't created a valid dataset for our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_size:6567\n",
      "reduced_size:6370\n",
      "compression:3.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "VOCAB_SIZE=100256 + 1 # cl100k_base final token\n",
    "MAX_TOKEN_LENGTH=128\n",
    "\n",
    "class Custom(Dataset):\n",
    "    def __init__(self, tokenizer, data_folder=INPUT_DATA_FOLDER, dataset_folder=DATASET_FOLDER, vocab_size=VOCAB_SIZE, max_token_length=MAX_TOKEN_LENGTH, save=True):\n",
    "        super().__init__()\n",
    "        self.max_token_length = max_token_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_folder = data_folder\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.pad_token_id = 0\n",
    "        \n",
    "        self.chunks = self.create_chunks()\n",
    "        \n",
    "        if save:\n",
    "            self.save()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __chunks(self):\n",
    "        def helper(text):\n",
    "            encoded_text = self.tokenizer.encode(text)\n",
    "            \n",
    "            chunks = []\n",
    "            for i in range(0, len(encoded_text), self.max_token_length):\n",
    "                upper_limit = i + self.max_token_length if i + self.max_token_length < len(encoded_text) else len(encoded_text)\n",
    "                input_tokens = encoded_text[i:upper_limit]\n",
    "                output_token = encoded_text[upper_limit] if upper_limit < len(encoded_text) else self.pad_token_id\n",
    "                \n",
    "                if len(input_tokens) < self.max_token_length:\n",
    "                    input_tokens += [self.pad_token_id] * (self.max_token_length - len(input_tokens))\n",
    "                \n",
    "                chunks.append((input_tokens, output_token))\n",
    "            \n",
    "            return chunks\n",
    "\n",
    "        chunks = []\n",
    "        for filename in os.listdir(self.data_folder):\n",
    "            text_file_loc = os.path.join(self.data_folder, filename)\n",
    "            \n",
    "            text = \"\"\n",
    "            with open(text_file_loc, \"r\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            for chunk in helper(text):\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def create_chunks(self, threshold:float=.85):\n",
    "        chunks = self.__chunks()\n",
    "        input_chunks = [chunk[0] for chunk in chunks]\n",
    "        output_chunks = [chunk[1] for chunk in chunks]\n",
    "        print(f'original_size:{len(chunks)}')\n",
    "        \n",
    "        # filter to remove similar sentences (VERY BASIC)\n",
    "        def normalize(vec):\n",
    "            norm = np.linalg.norm(vec)\n",
    "            return vec / norm if norm > 0 else vec\n",
    "        \n",
    "        normalized_inputs = [normalize(np.array(chunk)) for chunk in input_chunks]\n",
    "        \n",
    "        filtered_indices = []\n",
    "        for i in range(len(normalized_inputs)):\n",
    "            is_similar = False\n",
    "            for j in filtered_indices:\n",
    "                dot_product = np.dot(normalized_inputs[i], normalized_inputs[j])\n",
    "                if dot_product > threshold:\n",
    "                    is_similar = True\n",
    "                    break\n",
    "            if not is_similar:\n",
    "                filtered_indices.append(i)\n",
    "        \n",
    "        \n",
    "        filtered_input_chunks = [input_chunks[i] for i in filtered_indices]\n",
    "        filtered_output_chunks = [output_chunks[i] for i in filtered_indices]\n",
    "        \n",
    "        final_chunks = list(zip(filtered_input_chunks, filtered_output_chunks))\n",
    "        print(f'reduced_size:{len(final_chunks)}')\n",
    "        print(f'compression:{round(1.0 - (len(final_chunks) / len(chunks)), 4) * 100}%')\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def save(self,):\n",
    "        df = pd.DataFrame(columns=['input', 'output'])\n",
    "        \n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            df.loc[i] = [ chunk[0], chunk[1] ]\n",
    "        \n",
    "        df.to_csv(os.path.join(self.dataset_folder, 'dataset.csv'))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.chunks[index]\n",
    "\n",
    "\n",
    "dataset = Custom(tokenizer=get_encoding('cl100k_base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LEN = int(len(dataset) * 0.8)\n",
    "TEST_LEN = len(dataset) - TRAIN_LEN\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = dataset[0:TRAIN_LEN]\n",
    "test_dataset = dataset[TRAIN_LEN:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL:\n",
    "\n",
    "Here, since the model is a next word predictor, a simple decoder only architecture will suffice. For tasks such as translation, encoder is required to provide the keys and values for the queries produced by the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 100257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "VOCAB_SIZE=100256 + 1 # cl100k_base final token\n",
    "MAX_TOKEN_LENGTH=128\n",
    "NUM_EPOCHS=20\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=256, num_heads=8, num_layers=12, max_token_length=MAX_TOKEN_LENGTH):\n",
    "        super().__init__()\n",
    "        self.max_token_length = max_token_length\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)                                          # Token Embedding\n",
    "        self.position_embedding = nn.Embedding(max_token_length, d_model)                           # Positional Embedding\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)                                                # Final linear layer to predict token probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        positions = torch.arange(0, self.max_token_length, device=x.device).unsqueeze(0)    # [1, max_token_length]\n",
    "        # print(x.device, positions.device)\n",
    "        x = self.embedding(x) + self.position_embedding(positions)                          # [batch_size, max_token_length, d_model]\n",
    "        \n",
    "        # Masked Self-Attention\n",
    "        tgt_mask = self.generate_square_subsequent_mask(self.max_token_length).to(x.device) # [batch_size, batch_size]\n",
    "        output = self.decoder(tgt=x, memory=x, tgt_mask=tgt_mask)                           # [batch_size, max_token_length, d_model]\n",
    "        \n",
    "        logits = self.fc_out(output)                                                        # [batch_size, max_token_length, vocab_size] \n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "        \n",
    "\"\"\"\n",
    "TEST:\n",
    "\"\"\"\n",
    "\n",
    "def test_model(batch_size=8):\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    input_tokens = torch.randint(0, VOCAB_SIZE, (batch_size, MAX_TOKEN_LENGTH), dtype=torch.long).to(device)\n",
    "    model.train()\n",
    "    logits = model(input_tokens)\n",
    "    print(logits.shape)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (1): 100%|██████████| 657/657 [01:28<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.1707563400268555 | Accuracy: 6.41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.1707563400268555, 6.41)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def one_epoch(model, train_dataloader, loss_fn, optimizer, epoch_no):\n",
    "    total_correct, total_tokens = 0, 0\n",
    "    for input_tokens, output_token in tqdm(train_dataloader, desc=f\"Training Epoch ({epoch_no + 1})\"):\n",
    "        input_tokens = torch.stack(input_tokens, dim=1)\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        output_token = output_token.to(device)\n",
    "        \n",
    "        # final batch!\n",
    "        current_size = input_tokens.size(0)\n",
    "        if current_size != BATCH_SIZE:\n",
    "            padding_size = BATCH_SIZE - current_size\n",
    "            input_tokens = torch.cat([input_tokens, torch.zeros(padding_size, 128).to(device)], dim=0)\n",
    "            output_token = torch.cat([output_token, torch.zeros(padding_size).to(device)], dim=0)\n",
    "        \n",
    "        input_tokens = input_tokens.long()\n",
    "        output_token = output_token.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_tokens)\n",
    "        \n",
    "        prediction = logits[:, -1, :]\n",
    "        \n",
    "        loss = loss_fn(prediction, output_token)\n",
    "        \n",
    "        _, predicted_tokens = prediction.max(dim=1)\n",
    "        # print('Predictions: ', predicted_tokens, '\\nTarget:',output_token)\n",
    "        correct_tokens = (predicted_tokens == output_token).sum().item()\n",
    "        total_correct += correct_tokens\n",
    "        total_tokens += output_token.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    accuracy = round(total_correct / total_tokens, 4) * 100\n",
    "    print(f\"Loss: {loss.item()} | Accuracy: {accuracy}\")\n",
    "    return (loss.item(), accuracy)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "TEST:\n",
    "\"\"\"\n",
    "model = Model()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "one_epoch(model, train_dataloader, loss, optimizer, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (1): 100%|██████████| 657/657 [02:18<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.262206077575684 | Accuracy: 6.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (2): 100%|██████████| 657/657 [02:42<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.1240234375 | Accuracy: 6.260000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (3): 100%|██████████| 657/657 [02:40<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.116603374481201 | Accuracy: 6.279999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (4): 100%|██████████| 657/657 [02:48<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.120927095413208 | Accuracy: 6.370000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch (5):  27%|██▋       | 180/657 [00:45<01:59,  3.98it/s]"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "metrics = []\n",
    "\n",
    "os.makedirs('model_savepts', exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    metric = one_epoch(model, train_dataloader, loss_fn, optimizer, epoch)\n",
    "    metrics.append(metric)\n",
    "    torch.save(model.state_dict(), f'./model_savepts/train_epoch{epoch}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "accuracy = [ metric[1] for metric in metrics ]\n",
    "loss = [ metric[0] for metric in metrics ]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(accuracy, color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=20):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    if len(tokens) < MAX_TOKEN_LENGTH:\n",
    "        tokens += [0] * (MAX_TOKEN_LENGTH - len(tokens))\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    # print(input_ids.device)\n",
    "    print(input_ids.shape)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token = torch.argmax(logits[0, -1, :]).item()\n",
    "            next_token = torch.tensor([[next_token]]).to(device)\n",
    "            input_ids = torch.cat((input_ids[:, 1:], next_token), dim=1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "prompt = \"In Euclidean geometry, an angle is the figure formed by two rays\"\n",
    "output = generate_text(model, get_encoding('cl100k_base'), prompt, max_length=10)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
